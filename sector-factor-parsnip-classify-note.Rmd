---
title: "Sector Factor Note"
output:
  word_document: default
  html_notebook: default
---


```{r setup, include = FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(message=FALSE, warning=FALSE, comment = NA)
library(tidyverse)
library(broom)
library(tidyquant)

```

In this Reproducible Finance note, we will review and walk through the R code for importing price data from CSV, coercing it to monthly returns data, and then running a Fama French (FF) factor model on the data. That will also necessitate importing and wrangling the FF data before running the model and visualizing the results. The goal of this note is to introduce a reusable R code flow for taking two data sets, joining them together by a date column and then running a model. The work here is not overly complex but it should serve as a good starting point for more involved work in the future.[^1]

[^1]: This note was written using Rmarkdown, an R format that can create PDFs, HTML reports or Word documents. If you wish to download the full script that was used for this note and see the charts in color, you can do so here: www.reproduciblefinance.com/2019/03/05/sector-factor-jfds/

Let's get started by loading the packages that we will need for today's work. If you are unfamiliar with R, packages are a crucial part of the ecosystem and a big reason for the popularity of the language. They contain useful and intuitive functions that save us from having to write our own functions in base R. 

We will be using the following packages.

```{r, eval=FALSE}
# To import, wrangle and visualize
library(tidyverse)
# To work with financial time series
library(tidyquant)
# To clean up model results
library(broom)
```

With our packages loaded, we need to import our data. We will be working with and modeling the following SPDR sector ETFs.

```{r, echo = FALSE}

etf_ticker_sector <- tibble(
  ticker = c("XLY", "XLP", "XLE",	
          "XLF", "XLV",	"XLI", "XLB", 
          "XLK", "XLU",  "SPY"),	
  sector = c("Consumer Discretionary", "Consumer Staples", "Energy", 
          "Financials", "Health Care", "Industrials", "Materials", 
          "Information Technology", "Utilities", "Market")
)

etf_ticker_sector
```

We could import price data on those ETFs from a source like Factset or Bloomberg, but to make this fully reproducible, I have saved the price data as a CSV file that can be downloaded.

To import the data into our R working environment, we will use the `read_csv()` function and read directly from the server where I stored this data.


```{r}
sector_prices_2000_2018 <- 
read_csv("https://colorado.rstudio.com/rsc/content/2138/sector_prices_2000_2018.csv") %>% 
group_by(sector)
```

Next let's use the`dplyr` package and the `slice()` function to glance at the first price observation for each sector, to confirm that the data looks how we were expecting it to look. Note that the first date is December 29, 1999. I set that as the start date because we eventually want to capture the monthly return of January 2000 and we will index that off of the last day of December 1999.

```{r}
sector_prices_2000_2018 %>% 
  slice(1)
```


The ultimate goal is to regress the monthly sector returns on the 5 Fama French Global monthly factors, so let's coerce this daily price data to monthly price data.  

For that task, we use `tq_mutate` from the `tidyquant` package and set `mutate_fun = to.period, period = "months", indexAt    = "lastof"`, which will convert our daily data to monthly, indexed at the last day of each month. 

```{r}
sector_prices_2000_2018 %>%
  tq_transmute(select     = adjClose, 
               mutate_fun = to.period, 
               period     = "months",
               indexAt    = "lastof") %>% 
  head()
```


With our daily price data converted to monthly, we can calculate monthly log returns using `mutate(monthly_return = log(adjClose) - log(lag(adjClose)`. The `mutate()` function, from the `dplyr` package, adds a new column to the data and we set the name of the new column to be `monthly_return`. We calculate monthly returns as the log of the adjusted close on the final day of the current month, minus the log of the adjusted close on the final day of the previous month. This is for illustration purposes - your team might choose to use a price other than adjusted close and different date periodicities.

```{r}
sector_monthly_returns_2000_2018 <-
sector_prices_2000_2018 %>%
  tq_transmute(select     = adjClose, 
               mutate_fun = to.period, 
               period     = "months",
               indexAt    = "lastof") %>% 
  mutate(monthly_return = log(adjClose) - log(lag(adjClose))) %>% 
  na.omit() 

sector_monthly_returns_2000_2018 %>% 
  head()
```

We now have monthly returns for our sectors, starting in January of 2000, saved in a data object called `sector_monthly_returns_2000_2018`.

Let's split this into data for 2000 - 2017 and for 2018, in case we need a hold out set for validation later. 

```{r}
sector_monthly_returns_2000_2017 <- 
  sector_monthly_returns_2000_2018 %>% 
  filter(date < "2018-01-01")

sector_monthly_returns_2018 <- 
  sector_monthly_returns_2000_2018 %>% 
  filter(date >= "2018-01-01")
```


Next we need to import the Fama French (FF) factor data and, luckily, FF make their factor data available on their website.[^2]

[^2]: http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html

Have a quick glance at the website and notice that the factor data are packaged as zip files so we will need to do a bit more than call `read_csv()`.

We will use the `tempfile()` function from base R to create a variable called `temp`, and will store the zipped file there.

Then we invoke `download.file()` and pass it the URL address of the zip for the monthly global factors, which is “http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/Global_5_Factors_CSV.zip”.

```{r}
temp <- tempfile()

download.file(
  "http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/Global_5_Factors_CSV.zip",
  temp, 
  quiet = TRUE)
```

Next we unzip that data with the `unz()` function read and the csv file using `read_csv()`.

```{r}
Global_5_Factors <- 
  read_csv(unz(temp, "Global_5_Factors.csv"), skip =6) %>%
  rename(date = X1)
```

That loads the data into our environment but we need to do some wrangling to get this raw data into a format that we can join with our monthly returns data. When joining disparate financial data sets, we often need to use the date column and that means ensuring a common format.

Have a look at the date column for our FF factor data.

```{r}
Global_5_Factors %>% 
  select(date) %>% 
  head(2)
```

We want to join this data to our monthly returns using the date column, and that means we want the factor date to be in the same format as our monthly returns, but it's currently in a character string with the format `1990007` instead of `1990-07-31`. 

We can transform that string of numbers into date format with a call to `mutate(date = ymd(parse_date_time(date, "%Y%m")))`. The workhorse is the `parse_date_time()` function from the `lubridate` package. 

```{r}
Global_5_Factors %>%
  mutate(date = ymd(parse_date_time(date, "%Y%m"))) %>% 
  head()
```

That worked well for the date column, but note that all our factor columns have been read in as character strings (that why it says `<chr>` in the data frame above) and we want them to be numeric. We also need to coerce our date to be indexed at the end of the month because that's how our returns data is indexed. 

We coerce our character columns to numeric columns with `mutate_if(is.character, as.numeric)` and we change the index to the end of the month with `ceiling_date(date, unit = "month")`.

```{r}
Global_5_Factors_for_join <- 
  Global_5_Factors %>%
  mutate(date = ymd(parse_date_time(date, "%Y%m"))) %>% 
  mutate_if(is.character, as.numeric) %>% 
  mutate_if(is.numeric, funs(. / 100)) %>%
  mutate(date = ceiling_date(date, unit = "month") - days(1)) %>% 
  rename(MKT = `Mkt-RF`)

Global_5_Factors_for_join %>% 
  head()
```

The data frame looks good now, but that was quite a bit of work to create a nicely formatted date, indexed to the end of the month and to coerce the other columns to numeric instead of character data.  Wrangling dates is a crucial and time-consuming part of financial data work, especially when a project involves mashing together data sets from different sources.  The more comfortable and efficient we can get at transforming and aligning dates, the more quickly we can start to work with new and interesting data sources.  The next time we are confronted with a date string formatted like `199007`, we can reuse our code flow above to quickly convert it to another format.

We now have the Fama French factor cleaned and saved in an object called `Global_5_Factors_for_join`.  The next task is to regress our sector monthly returns on those factors. Let's first combine our sector monthly returns data with our factors data into one data object. We will join the data sets by their common piece of data, the `date` column. From a code perspective, we call `left_join(by = "date")`, so that only the factor and returns observations with the same exact dates will remain in the data frame. 

```{r}
sector_factor_data_joined <- 
sector_monthly_returns_2000_2017 %>% 
  left_join(Global_5_Factors_for_join, by = "date")
```

Here is a look at just the `date`, `monthly_returns` and factors from our joined data object which is called `sector_factor_data_joined`.

```{r}
sector_factor_data_joined %>%
  ungroup() %>% 
 select(-sector,  -adjClose, -RF)
```

Our data is now in a nicely structured data frame, grouped by sector, with sector monthly returns in one column, and each of the Fama French factors in a separate column. From here, we want to run our factor analysis, which we do by regressing our monthly returns on the 5 factors. 

For this note, let's run the regression on just the `Financials` and `Energy` sectors by first calling `filter(sector == "Financials" | sector == "Energy")`. If you wish to reproduce this work but run the regression on a different sector, you can change the code to `filter(sector == your sector of choice)`. If you wish to reproduce this work but run the regression on all the sectors, remove the `filter()` line of code altogether.

After filtering, we use `group_map()` (a brand new function added to the `dplyr` package in February of 2019) to apply a function to each group, which for for our purposes means running a function on each sector. The function we want to apply is a linear model, so we then pass `~lm(monthly_return ~ MKT + SMB + HML + RMW + CMA)`, and extract the model results, with coefficient confidence intervals, using `tidy(conf.int = T, conf.level = .95)`.


```{r}
ff_results <- 
  sector_factor_data_joined %>%
  filter(sector == "Financials" | sector == "Energy") %>%
  group_map(~lm(monthly_return ~ MKT + SMB + HML + RMW + CMA, 
                data = .x) 
            %>% tidy(conf.int = T, conf.level = .95)) %>% 
  rename(coefficient = estimate, factor = term)
```

`ff_results` holds the results of the 5-factor model that we just ran on the `Financials` and `Energy` sectors. Think about how this code flow could be scaled to easily accommodate dozens of groups or classifications of assets to be modeled.

Let's take a look at just the results for `Financials` sector.

```{r}
ff_results %>%
  filter(sector == "Financials") %>% 
  select(coefficient, conf.low, conf.high, p.value) %>% 
  # Round to 4 decimal places so the results fit on the page
  mutate_if(is.numeric, ~round(., 4))
```

We can finish our work with a couple of charts. Let's first show the coefficients and confidence intervals for our model of the `Financials` sector. We will use `ggplot()` to build this chart, mapping our factor names to the x-axis and our coefficients to the y-axis with `aes(x = factor, y = coefficient)`. Let's place a point for each coefficient by calling `geom_point()` and add confidence intervals with `geom_errorbar()`.

```{r}
ff_results %>% 
filter(sector == "Financials" & 
         factor != "(Intercept)") %>% 
  ggplot(aes(x = factor, 
             y = coefficient, 
             shape = factor, 
             color = factor)) + 
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, 
                    ymax = conf.high)) +
  labs(title = "FF 5-Factor Betas with Conf Intervals",
       x = "",
       y = "coefficients",
       caption = "data source: Fama-French website") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        plot.caption  = element_text(hjust = 0))
```

We can glance at this and quickly see that the MKT and HML factors have positive coefficients near 1, whereas both RMW and SMB have negative coefficients.  

If we wish to chart both sectors, we can use `facet_wrap(~sector)` to create two charts.

```{r}
ff_results %>% 
filter(
         factor != "(Intercept)") %>% 
  ggplot(aes(x = factor, 
             y = coefficient, 
             shape = factor, 
             color = factor)) + 
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, 
                    ymax = conf.high)) +
  labs(title = "FF 5-Factor Betas with Conf Intervals",
       x = "",
       y = "coefficients",
       caption = "data source: Fama-French website") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        plot.caption  = element_text(hjust = 0),
        panel.spacing = unit(2, "lines")) +
  facet_wrap(~sector) 
```

Remember that we ran the regression on the `Financials` and `Energy` sectors but we have monthly returns for several more ETFs. We could expand this work by filtering on a different sector or by adding more custom factors to the data frame before running the regression - that would probably require creating the factors and making sure the dates align. We could also convert our regression model to a rolling regression, to see where we might have missed any significant regime or market shifts, and then visualize our rolling factor coefficients.


Now let's take our same data object, holding monthly returns and factor data, and examine how we could run it through a resampling and machine learning workflow. We will start by reducing the data down to just the financials sector.

```{r}
financials_factor_data <- 
sector_factor_data_joined %>%
  filter(sector == "Financials")
```

Next, we want to prepare this 17 years of data to be run through a random forest model and then assess how well that model can fit the factor data to the monthly returns.  This is frequently done by k-fold cross validation but since we are working with time series data, we will use a time-aware technique for subsetting this data into training and test sets. We will turn to the `rsample` package for this work and use the `rolling_origin` function to create several training and the testing sets, though `rsample` labels them as `analysis` and `assessment` sets.  

We have monthly data from 2000 to 2017, so let's create analysis sets of 6 months and assessment sets of 1 month. In other words, we will run our model on 6 months of data and test how accurately the model results fit the next month. To accomplish that, the `rolling_origin()` function needs a few arguments set. We first set `data` to be `financials_factor_data` Then we assign `initial` to be `6` - this tells the function that the size of our first sample is 6 months. Our first chunk of `analysis` data will be the first 6 months of financial sector returns. Next we assign `assess` to be `1` - this tells the function that our `assessment` data is the 1 month of sector returns following those 6 months. Finally, we set `cumulative` to be `FALSE` - this tells the functions that each of splits is 6 months The first split is the first 6 months. If we were to set `cumulative = TRUE`, the first split would be 6 months. The next split would be 7 months, the next split would be 8 months. And so on. The `analysis` split months would be accumulating. For now, we will leave it at `cumulative = FALSE`.

```{r}
library(rsample)

financials_splits_2000_2017 <- 
 rolling_origin(
  data       = financials_factor_data,
  initial    = 6,
  assess     = 1,
  cumulative = FALSE
)

financials_splits_2000_2017 %>% 
  head()
```

Look at an individual split.

```{r}
 
  financials_splits_2000_2017$splits[[1]]

```

That `6` is telling us there are 6 months in the `analysis` set; that `1` is telling us that there is 1 month in our `assessment` data.

Here is the `analysis` subset of that split - it is 6 observations of monthly data.

```{r}
analysis(financials_splits_2000_2017$splits[[1]]) %>% 
  select(date, monthly_return)
```


And the `assessment` subset - this is 1 month (specifically, it's the 7th month of our data).

```{r}
assessment(financials_splits_2000_2017$splits[[1]]) %>% 
  select(date, monthly_return)

```

Now we can start using that collection of data splits to fit and then assess our model and that means it's time to introduce a relatively new addition to the R toolchain, the `parsnip` package.

`parsnip` is a unified model interface that allows us to create a model specification, set an analytical engine and then fit a model. It's a 'unified' interface in the sense that we can use the same scaffolding but insert different models, or different engines, or different modes. We will see how that works by toggling from a random forest regression model to a random forest classification model.  

We will go with `ranger` as the engine for our random forest model though it would be simple to change over to a different engine, such as the `randomForest` package.

To set up our model in `parsnip`, we first use `rand_forest(mode = "regression", mtry = 3, trees = 100)` to create the specification, `set_engine("ranger")` to set the engine as the `ranger` package, and `fit(monthly_return ~ MKT + SMB + HML + RMW + CMA ~ , data = analysis(financials_splits_2000_2017$splits[[1]]))` to fit the 5-factor Fama French model to the 6-month sample in our first split. 

```{r}
library(parsnip)
# Need to load the packages to be used as the random forest engine
library(ranger)

rand_forest(mode = "regression", mtry = 3, trees = 100) %>%
  set_engine("ranger") %>%
  fit(monthly_return ~ MKT + SMB + HML + RMW + CMA, 
      data = analysis(financials_splits_2000_2017$splits[[1]]))
      
```

Notice that `ranger` gives us an `MSE` value as part of its return. `parsnip` returns to us what the underlying engine returns.

Now, let's apply that random forest regression to all 210 of our splits, so we can get an average RMSE.

To do so, we create a function that takes one split, passes it to our `parsnip` enabled model, and then uses the `predict` function to attemtp to predict our `assessment` split.  The function also allows us to specify the number of trees and the number of variables randomly sampled at each tree split, which is set with the `mtry` argument.

```{r}

rf_regress <- function(mtry = 3, trees = 5, split){
    
    analysis_set_rf <- analysis(split)
     
    model <- 
      rand_forest(mtry = mtry, trees = trees) %>%
        set_engine("ranger", importance = 'impurity') %>%
        fit(monthly_return ~ MKT + SMB + HML + RMW + CMA, data = analysis_set_rf)

    
    assessment_set_rf <- assessment(split)


    assessment_set_rf %>%
      select(monthly_return) %>%
      mutate(.pred = unlist(predict(model, new_data = assessment_set_rf))) %>% 
      select(monthly_return, .pred)
   
}

```

Now we want to pass it our object of 210 splits, `financials_splits_2000_2017`, and we want the function to iterate over each split. For that we turn to `map_df()` from the `purrr` package, which allows us to iterate over the data object and return a data frame. `map_df()` takes the data as an argument and the function as an argument.

```{r}
results_rf <- map_df(.x = financials_splits_2000_2017$splits,
                      ~rf_regress(mtry = 3, trees = 200, split = .x))
```

Here are the results. We now have 210 predictions.

```{r}
results_rf %>% 
  head()
```

It's not necessary but I like to add the date of each prediction to the data as well. We'll create a function called `get_prediction_date()` and then apply it to the data.

```{r}
get_prediction_date <- function(x) 
  min(assessment(x)$date)

results_rf %>% 
  mutate(pred_date = map(financials_splits_2000_2017$splits, get_prediction_date) %>% reduce(c)) %>%
  select(pred_date, everything()) %>% 
  head()
```

Now we can use the `rmse()` function from `yardstick` to calculate the root mean-squared error each of our predictions. We can find the average rmse by calling `summarise(avg_rmse = mean(.estimate))`.

```{r}
library(yardstick)
results_rf %>%
  mutate(pred_date = map(financials_splits_2000_2017$splits, get_prediction_date) %>% reduce(c)) %>%
  select(pred_date, everything()) %>%
  group_by(pred_date) %>% 
  rmse(monthly_return, .pred) %>% 
  summarise(avg_rmse = mean(.estimate))
```


```{r}
financials_factor_data_labeled <- 
  financials_factor_data %>% 
  mutate(label = case_when(monthly_return > 0 ~ "positive",
                           monthly_return <= 0 ~ "negative"))


fin_labeled_splits_2000_2017 <- 
 rolling_origin(
  data       = financials_factor_data_labeled,
  initial    = 6,
  assess     = 1,
  cumulative = FALSE
)

fin_labeled_splits_2000_2017
```

```{r}
rf_class_mod <- 
rand_forest(mode = "classification", mtry = 3, trees = 100) %>%
  set_engine("ranger", importance = "impurity") %>%
  fit(label ~ MKT + SMB + HML + RMW + CMA, 
      data = analysis(fin_labeled_splits_2000_2017$splits[[10]]))

predict(rf_class_mod, new_data = assessment(fin_labeled_splits_2000_2017$splits[[1]]), type = "prob")
```


```{r}

rf_class<- function(mtry = 3, trees = 5, split, id){
    
    analysis_set_rf <- analysis(split)
     
    model <- 
      rand_forest(mode = "classification",, mtry = mtry, trees = trees) %>%
        set_engine("ranger", importance = 'impurity') %>%
        fit(label ~ MKT + SMB + HML + RMW + CMA, data = analysis_set_rf)

    
    assessment_set_rf <- assessment(split)

assessment_set_rf %>% 
  bind_cols(
   predict(model, new_data = assessment_set_rf, type = "prob")
  )
}

```


```{r}
results_rf_class <- map2_df(.x = fin_labeled_splits_2000_2017$splits,
                      .y = fin_labeled_splits_2000_2017$id,
                      ~rf_class(mtry = 3, trees = 200, split = .x, id = .y))

results_rf_class %>% 
  select(date, monthly_return, label, .pred_negative, .pred_positive)
```

